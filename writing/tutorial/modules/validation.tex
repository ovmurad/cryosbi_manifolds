% ======================================================================
\section{SBI Validation Pipeline}
% ======================================================================
% ----------------------------------------------------------------------
% Overview: Evaluate Coverage and Fidelity
% ----------------------------------------------------------------------
\begin{frame}[t]{SBI Validation Pipeline: Evaluate Coverage and Fidelity}

\footnotesize

\begin{columns}[t]
	
	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Shared Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
		\end{itemize}
	\end{column}

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Specific Step}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cPurple!15}{\textbf{Step 4.} Assess Coverage and Fidelity.}
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Overview: Intrinsic Dimensionality Estimation
% ----------------------------------------------------------------------
\begin{frame}[t]{SBI Validation Pipeline: Intrinsic Dimensionality Estimation}

\footnotesize

\begin{columns}[t]

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Shared Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 5.} Preprocessing II: Uniform Resampling.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
		\end{itemize}
	\end{column}

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Specific Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cGreen!15}{\textbf{Step 8.} Recompute Local Statistics.}
			\item[]
			\stepbox{cGreen!15}{\textbf{Step 9.} Estimate intrinsic dimension $d$.}
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Overview: Manifold Interpretation and Visualization
% ----------------------------------------------------------------------
\begin{frame}[t]{SBI Validation Pipeline: Manifold Interpretation \& Visualization}

\footnotesize

\begin{columns}[t]

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Shared Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 5.} Preprocessing II: Uniform Resampling.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
		\end{itemize}
	\end{column}

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Specific Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 8a.} Manifold Interpretation \& Visualization I: \ies\ and plotting.}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 8b.} Manifold Interpretation \& Visualization II: \tslasso.}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 8c.} Manifold Interpretation \& Visualization III: \rrelex.}
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

% ======================================================================
% Evaluate Coverage and Fidelity
% ======================================================================
% ----------------------------------------------------------------------
% Overview
% ----------------------------------------------------------------------
% TODO: Could add a schematic showing coverage gaps vs good coverage
% TODO: Fidelity assessment is not covered in the paper, and might be something to remove for now, but could be a future direction.
% TODO: If we do end up talking about fidelity, should add picture of coverage and fidelity visualizations.
\begin{frame}{Evaluate Coverage and Fidelity: Overview}

	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
				\item[]
				\stepbox{cPurple!15}{\textbf{Step 4.} Assess Coverage and Fidelity.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Goal:} Compare distributions $p_s$(simulation) vs. $p_e$(experimental) by assessing:
			\begin{itemize}
				\item \textbf{Coverage}: Whether $p_s$ "covers" $p_e$(i.e. simulation generalizes to experimental data).
				\item \textbf{Fidelity}: Whether $p_e$ "covers" $p_s$(i.e. simulation produces realistic data).
			\end{itemize}

			\textbf{Why this matters for SBI:} If $p_s$ and $p_e$ are not similar, then:
			\begin{itemize}
				\item SBI posteriors trained on data from $p_s$ may be mis-calibrated.
				\item Simulator might need refinement.
			\end{itemize}
		\end{column}

	\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Step 1: Distance Matrix and Local Statistics
% ----------------------------------------------------------------------
\begin{frame}{Step 1: Construct Distance Matrix $\distmat$}

	\textbf{Optional: Sub-sampling for tractability}
	\begin{itemize}
        \small
		\item Many downstream operations scale super-linearly in $\N$, the number of data points:
        \item[] \begin{itemize}
            \item Pairwise distances / neighbor graphs,
            \item Spectral decompositions of graph Laplacians,
            \item Local statistics(distance to $k$-nearest neighbors, number of neighbors within radius $r$).
        \end{itemize}
		\item If $\N$ is very large, randomly sub-sample to a size that:
        \item[] \begin{itemize}
            \item Still captures the geometry of the data, % TODO: Could this be clarified?
            \item Fits within the available computational budget.
        \end{itemize}
		\item Use as many points as resources allow.
	\end{itemize}

	\textbf{Core Step: Distance matrix}
	\begin{itemize}
        \small
		\item Compute distances between points in $\X$.
		\item Store distances in sparse form by only storing the distances between nearby points.
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Step 2: Compute Local Statistics
% ----------------------------------------------------------------------
\begin{frame}{Step 2: Compute Local Statistics}

	\textbf{For each point $x \in \X$, compute either:}
	\begin{itemize}
        \small
		\item $\dk[x]$: Distance to the $k$-th nearest neighbor for a range $k \in \{k_{\min}, \dots, k_{\max}\}$.
		\item[]\begin{center} {\LARGE or} \end{center}
		\item $\nr[x]$: The number of neighbors within radius $r$ for a range $r \in \{r_{\min}, \dots, r_{\max}\}$.
	\end{itemize}

	% \textbf{Why the distance matrix is central:}
	% \begin{itemize}
    %     \small
	% 	\item Construction of the affinity matrix $\affmat$ and graph Laplacian $\lapmat$.
	% 	\item Local density estimates via local statistics.
	% 	\item Intrinsic dimensionality estimators via local statistics.
	% 	\item Tangent-space estimation via local covariance.
	% \end{itemize}

	\textbf{Practical notes:}
	\begin{itemize}
        \small
		\item Use approximate nearest neighbor methods for large $\N$.
		\item Reuse the same local statistics across multiple steps in the pipeline.
	\end{itemize}

\end{frame}


% ----------------------------------------------------------------------
% Step 3: Outlier Removal
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
% TODO: Could show before/after outlier removal visualization.
% TODO: Could do an ablation study of embedding methods with and without outlier removal.
% TODO: Could show a plot of mvs volumes for different alphas.
\begin{frame}{Step 3: Preprocessing I: Outlier Removal}

	\textbf{Motivation}: Highly noisy or pathological points:
    \begin{itemize}
        \small
        \item Distort local geometry,
        \item Destabilize spectral embeddings,
        \item Bias intrinsic dimension estimates.
    \end{itemize}

    \textbf{Goal}: {\small We want a \emph{clean} subset $\X[clean] \subseteq \X$ for all downstream operations.}

	\textbf{Recommended approach - \mvs(\mvs*) \citep{TODO}}:
	\begin{itemize}
        \small
		\item Robust and theoretically grounded.
		\item Uses the same local statistics used later in the pipeline for dimensionality estimation.
	\end{itemize}

    \textbf{But good outlier removal works}: {\small One-Class SVMs, Isolation Forests, DBSCAN, etc.}

\end{frame}

% ----------------------------------------------------------------------
% TODO: Show histogram of density scores and selection of alpha threshold
\begin{frame}{Outlier Removal with \mvs(\mvs*) \citep{TODO}}

	\textbf{How it works theoretically}:
	\begin{itemize}
        \small
		\item For $\alpha \in (0,1)$, define an $\alpha$-\mvs*\ as the smallest set containing at least fraction $\alpha$ of the probability mass.
		\item Flag points outside the $\alpha$-\mvs*\ as outliers and remove them.
	\end{itemize}

	\textbf{In practice, we can estimate an $\alpha$-\mvs*\ with neighbor-based scores}:
	\begin{itemize}
		\small
		\item For each point $x$:
			\begin{itemize}
				\item Compute a relative density score (e.g.\ $1/\dk[x]$ or $\nr[x]$),
				\item Rank all points by this score.
			\end{itemize}
		\item Keep roughly the top $\alpha \N$ highest-density points as in-distribution,
		\item Discard the remaining $(1-\alpha) \N$ as outliers.
		\item Optionally average rankings over multiple $k$ / $r$ for robustness.
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Step 4: Coverage and Fidelity Assessment
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\begin{frame}{Step 4: Assess Coverage and Fidelity}

	\textbf{Goal:} Verify whether $p_s$(simulation) $\approx p_e$(experimental) by assessing:
	\begin{itemize}
		\small
		\item \textbf{Coverage}: whether $p_s$ ``covers'' $p_e$ (i.e.\ simulations generalize to experimental data).
		\item \textbf{Fidelity}: whether $p_e$ ``covers'' $p_s$ (i.e.\ simulations produce realistic data).
	\end{itemize}
	
	\textbf{How it works theoretically:}
	\begin{itemize}
		\small
		\item $D_{\mathrm{KL}}(p_e \parallel p_s)$ evaluates \textbf{coverage}: high values $\Rightarrow$ poor coverage.
		\item $D_{\mathrm{KL}}(p_s \parallel p_e)$ evaluates \textbf{fidelity}: high values $\Rightarrow$ poor fidelity.
	\end{itemize}
	
	\textbf{How it works in practice:}
	\begin{itemize}
		\small
		\item Use training subsets of $\X[s][clean]$ and $\X[e][clean]$ to estimate density models $\hat{p}_e$ and $\hat{p}_s$.
		\item Estimate divergences via Monte Carlo integration using held-out samples.
	\end{itemize}
	
\end{frame}

% ----------------------------------------------------------------------
\begin{frame}{Assessing Coverage and Fidelity with KDEs}

	\textbf{Recommended approach - \kde(\kde*) \citep{TODO}:}
	\begin{itemize}
		\footnotesize
		\item Uses matrices we already have from the preprocessing steps.
		\item Naturally supports Monte Carlo estimation of KL-divergences.
		\item Performance adapts and improves as the intrinsic data dimension decreases.
	\end{itemize}

	\textbf{Density estimation via \kde*:}
	\begin{itemize}
		\footnotesize
		\item For a dataset $\X = \{x_i\}_{i=1}^{\N} \sim p$, kernel function $K_h(\cdot)$, bandwidth $h$, the \kde*\ is given by:
		\[ 
			\hat{p}(x) = \frac{1}{\N} \sum_{i=1}^{\N} K_h(x - x_i) 
		\]
		\item We split $\X[s][clean]$ and $\X[e][clean]$ into training and test sets.
		\item We use the training data to fit models $\hat{p}_e$ and $\hat{p}_s$ and select the bandwidth $h$ via validation.
		\item The test sets are used for Monte Carlo KL estimation:
		\item[] 
			\begin{itemize}
			\footnotesize
			\item \textbf{Coverage} is assessed using
			\(
				D_{\mathrm{KL}}(p_e \parallel p_s)
				\approx
				\frac{1}{|\X[e][test]|}
				\sum_{x_e \in \X[e][test]}
				\Big(
					\log \hat{p}_e(x_e)
					-
					\log \hat{p}_s(x_e)
				\Big)
			\).
			\item \textbf{Fidelity} uses the reverse divergence $D_{\mathrm{KL}}(p_s \parallel p_e)$ which is estimated analogously.
			\end{itemize}
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Summary
% ----------------------------------------------------------------------
\begin{frame}{Summary: Evaluate Coverage and Fidelity}

	\begin{columns}[t]

		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
				\item[]
				\stepbox{cPurple!15}{\textbf{Step 4.} Assess Coverage and Fidelity.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Key takeaways:}
			\begin{itemize}
				\item \textbf{Coverage} and \textbf{Fidelity} formalize mismatch between simulated and experimental.
				\item We distinguish two failure modes:
				\begin{itemize}
					\item lack of \textbf{coverage} (real data lie outside the support of simulations).
					\item lack of \textbf{fidelity} (simulations generate unrealistic samples).
				\end{itemize}
				\item Assessing these properties tests whether the learned representation is suitable for downstream SBI.
			\end{itemize}
		\end{column}

	\end{columns}

\end{frame}

% ======================================================================
% Intrinsic Dimensionality Estimation
% ======================================================================
% ----------------------------------------------------------------------
% Overview
% ----------------------------------------------------------------------
\begin{frame}{Intrinsic Dimensionality Estimation: Overview}

	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Preprocessing II: Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cGreen!15}{\textbf{Step 8.} Estimate intrinsic dimension $d$.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			
			\small

			\textbf{Goal:} Estimate the intrinsic dimension $d$ of the simulation and experimental manifolds $\M_s$ and $\M_e$. 
			Use multiple estimators with different parameters to provide a robust estimate of $d$.

			\vspace{0.5em}

			\textbf{Why this matters for SBI:}
			\begin{itemize}
				\item Check if the simulation and experimental data manifolds have similar intrinsic dimensions.
				\item If working with neural network embeddings, check that the network has learned a low-dimensional representation of the data.
				\item Understanding the complexity of the data distribution.
				\item Guiding downstream analysis and visualization.
			\end{itemize}

		\end{column}
	\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Step 4: Recompute Local Statistics
% ----------------------------------------------------------------------
\begin{frame}{Step 4: Recompute Local Statistics}

	\small
	We use the same steps as in the previous section to remove outliers from the data and obtain the clean subset $\X[clean]$:
	\begin{itemize}
		\setlength\itemsep{1pt}
		\item[]
		\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
		\item[]
		\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
		\item[]
		\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
	\end{itemize}

	\textbf{This ensures that the local statistics reflect the geometry of the clean data and are not biased/distorted by the outliers.}

\end{frame}

% ----------------------------------------------------------------------
% Step 5: Preprocessing II: Uniform Resampling
% ----------------------------------------------------------------------
% TODO: Could show density distribution before/after resampling or picture from the paper
\begin{frame}{Step 5: Preprocessing II: Uniform Resampling}

	\small

	\textbf{Goal:} Resample the data to imitate a sample from a uniform distribution over $\M$.

	\textbf{Reasoning:}
	\begin{itemize}
		\item A non-uniform data distribution over $\M$ entangles geometric structure with distribution artifacts. Specifically, non-uniform densities:
		\begin{itemize}
			\item Bias estimators of the Laplacian-Beltrami Operator $\lapop$.
			\item Bias the estimation of the intrinsic dimension $d$.
			\item Complicate neighborhood and bandwidth selection.
		\end{itemize}
		\item Most geometric methods assume or perform better with a uniform data distribution.
	\end{itemize}

	\textbf{Solution: Inverse-density weighted resampling}
	\begin{itemize}
		\item Estimate local density $\hat{p}(x)$ for $x \in \X[clean]$ using local statistics.
		\item Define sampling weights $\propto 1 / \hat{p}(x)$.
		\item Sample (without replacement) a subset $\X[unif]$ using these weights.
	\end{itemize}
	
\end{frame}


% ----------------------------------------------------------------------
% Step 6: Construct the Laplacian Matrix $\lapmat$
% ----------------------------------------------------------------------
\begin{frame}{Step 6: Construct the Laplacian Matrix $\lapmat$}

	\small
	
	\textbf{Construction of the affinity matrix $\affmat$}:
	\begin{itemize}
		\item On $\X[unif]$, build $\affmat$ by:
			\begin{itemize}
				\item Choosing a neighborhood scheme ($k$-NN or cutoff radius $r$),
				\item Defining edge weights using a kernel (e.g. Gaussian with bandwidth $\eps$).
			\end{itemize}
		\item \textbf{Strongly recommended}: Select bandwidth and cutoff scales by a geometry-preservation criterion(e.g.\ distortion minimization \citep{MeilaRMetric}).
	\end{itemize}

	\vspace{0.7em}

	\textbf{Construction of the Laplacian matrix $\lapmat$}:
	\begin{itemize}
		\item \textbf{Recommended}: Discard points with low degree to stabilize eigen-decompositions of $\lapmat$ further in the pipeline.
		\item Compute the renormalized Laplacian of \cite{CoifmanDM}.
	\end{itemize}

	\textbf{Important:} Both $\affmat$ and $\lapmat$ should be sparse matrices to save resources.

\end{frame}

% ----------------------------------------------------------------------
% Step 7: Manifold Learning: \diffmap
% ----------------------------------------------------------------------
% TODO: Could show eigenvalue spectrum to verify smooth, connected manifold
\begin{frame}{Step 7: Manifold Learning: \diffmap \citep{CoifmanDM}}

	\small
	\textbf{Goal:} Compute low-dimensional embedding coordinates $\Phi$ of the data points.

	\begin{itemize}
		\item Compute the eigen-decomposition of $\lapmat$.
		\item Choose the embedding dimension $m$ to comfortably larger than the intrinsic dimension $d$.
		\item Keep theRecompute $m$ smallest non-trivial eigenvalues and corresponding eigenvectors: $\lambda_{1:m}, \Phi_{1:m}$. 
		\item Interpreting the spectrum:
	\end{itemize}

	\textbf{Interpreting the spectrum, $\lambda_{1:m}$:}
	\begin{itemize}
		\item Single zero eigenvalue $\Rightarrow$ connected manifold,
		\item Slow growth of eigenvalues $\Rightarrow$ smooth geometry.
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Step 8: Estimate Intrinsic Dimension $d$
% ----------------------------------------------------------------------
\begin{frame}{Step 8: Estimate Intrinsic Dimension $d$}

	\small

	\textbf{Use multiple estimators to estimate the intrinsic dimension $d$:}
	\begin{itemize}
	
		\item \textbf{Correlation dimension} \citep{ProcacciaSlope}
			\begin{itemize}
				\item Scaling of average neighbor counts with radius $r$: $\log \nr[x] \propto d \cdot \log r$.
				\item Global estimate: $\hat{d} = \text{slope of } \log \overline{\nr[x]} \text{ vs. } \log r$.
			\end{itemize}
		\item \textbf{Doubling dimension} \citep{AssouadDoubling}
			\begin{itemize}
				\item By the same principle we have the local estimate $\hat{d}_r(x) = \log_2 \bigl(\nr[x][2r]/\nr[x][r]\bigr)$.
			\end{itemize}
		\item \textbf{Eigengap method} \citep{MaggioniEigenGap}
			\begin{itemize}
				\item Compute local covariance matrices and their eigen-decompositions.
				\item Local estimates: $\hat{d}(x) = \arg\max_{d} \lambda_{d} - \lambda_{d+1}$.
			\end{itemize}
		\item \textbf{MLE-based estimators} \citep{BickelLB}
			\begin{itemize}
				\item Use scaling of $\dk[x]$ for the top $k$-NN to locally estimate $d$ as:
				$\hat{d}_k(x) = \left[ \frac{1}{k-1} \sum_{j=1}^{k-1} \log \left(\frac{\dk[x][k]}{\dk[x][j]}\right) \right]^{-1}$.
			\end{itemize}
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Intrinsic Dimensionality Estimation Considerations
% ----------------------------------------------------------------------
% TODO: Add figures from paper showing stability of estimates across parameter ranges
\begin{frame}{Intrinsic Dimensionality Estimation Considerations}
	
	\small

	\textbf{Important:} Because many estimators of the intrinsic dimension $d$ require $\nr[x]$ or $\dk[x]$, we need to recompute(again!) them on $\X[unif]$.

	\textbf{Robustness:}
	\begin{itemize}
		\item For local estimators, we can average estimates $\hat{d}(x)$ to obtain a global estimate $\hat{d}$.
		\item For local estimators also depending on $k$ or $r$, we can further average over a range of values for $k$ or $r$.
		\item For estimators depending on $k$ or $r$, we need to select ranges $k_{\min}, k_{\max}$ and $r_{\min}, r_{\max}$ that have stable estimates.
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Summary
% ----------------------------------------------------------------------
\begin{frame}{Summary: Intrinsic Dimensionality Estimation}
	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Preprocessing II: Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cGreen!15}{\textbf{Step 8.} Estimate intrinsic dimension $d$.}
			\end{itemize}
		\end{column}
		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Key points:}
			\begin{itemize}
				\item \textbf{Uniform resampling} is crucial for unbiased and reliable dimension and Laplacian estimation.
				\item Recompute neighbor statistics (\(\nr[x]\), \(\dk[x]\)) whenever the data is modified.
				\item Apply \textbf{multiple estimators} to robustly estimate intrinsic dimension $d$.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}


% ======================================================================
% Manifold Interpretation and Visualization
% ======================================================================
% ----------------------------------------------------------------------
% Overview
% ----------------------------------------------------------------------
\begin{frame}{Manifold Interpretation \& Visualization: Overview}
	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Preprocessing II: Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8a.} IES and plotting.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8b.} \tslasso.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8c.} \rrelex.}
			\end{itemize}
		\end{column}
		\begin{column}[t]{0.7\textwidth}
			\textbf{Goal:} Understand what factors parametrize the learned manifold representation.
			\vspace{0.5em}
			\begin{itemize}
				\item Identify which generative parameters or factors drive the geometry.
				\item Visualize the manifold structure in interpretable coordinates.
				\item Validate that the manifold captures relevant scientific parameters 
					rather than nuisance variation or noise.
			\end{itemize}
			\vspace{0.7em}
			\textbf{Three complementary approaches:}
			\begin{itemize}
				\item \ies: Select independent embedding coordinates for visualization.
				\item \tslasso: Identify which factors parametrize the tangent spaces.
				\item \rrelex: Refine embeddings to better preserve local geometry.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

% ----------------------------------------------------------------------
% Subsection: Independent Coordinate Selection
% ----------------------------------------------------------------------
\begin{frame}{Step 8a: Manifold Interpretation \& Visualization I: \ies\ and Plotting}
	\textbf{Problem:} spectral embeddings can produce dependent coordinates.
	\begin{itemize}
		\item Example: $(x, \sin x)$ varies in two directions but has only one DoF.
		\item Plotting all coordinates may be redundant or misleading.
	\end{itemize}
	\vspace{0.7em}
	\textbf{Independent Eigencoordinate Selection (\ies) \citep{MeilaIES}}
	\begin{itemize}
		\item Input: initial embedding matrix $\Phi$ from \diffmap.
		\item Output: subset $S$ of coordinates:
			\begin{itemize}
				\item As independent as possible,
				\item Low frequency (smooth) and locally full rank.
			\end{itemize}
		\item Typically choose $|S|$ around the estimated $\hat{d}$.
	\end{itemize}
\end{frame}

\begin{frame}{Visualization of Selected Coordinates}
	\textbf{Visualization of selected coordinates}
	\begin{itemize}
		\item Scatter plots of $\Phi_S$:
			\begin{itemize}
				\item 2D / 3D plots of the most informative coordinates,
				\item Color by generative parameters, SNR, or other labels.
			\end{itemize}
		\item Check for:
			\begin{itemize}
				\item Smooth variation with respect to relevant parameters,
				\item Separation of distinct regimes or modes,
				\item Potential artifacts (holes, disconnected components, etc.).
			\end{itemize}
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Subsection: Tangent Space Lasso
% ----------------------------------------------------------------------
\begin{frame}{Step 8b: Manifold Interpretation \& Visualization II: \tslasso \citep{MeilaTSLasso}}
	\textbf{Setup}
	\begin{itemize}
		\item Assume a dictionary of candidate functions $\{f_k\}$ on the manifold:
			\begin{itemize}
				\item Generative parameters, nuisance factors, or other variables of interest.
			\end{itemize}
		\item For each $f_k$:
			\begin{itemize}
				\item Estimate gradients $\nabla f_k$ in latent space
					via local regression / finite differences,
				\item Project gradients onto estimated local tangent spaces.
			\end{itemize}
	\end{itemize}
	\vspace{0.7em}
	\textbf{TSLasso idea}
	\begin{itemize}
		\item Reconstruct tangent spaces as sparse linear combinations of gradient fields.
		\item Apply a group-lasso penalty to encourage selection of a small subset of $\{f_k\}$.
		\item Sweep the regularization parameter to identify:
			\begin{itemize}
				\item Which factors consistently span the geometry,
				\item How strongly each factor contributes.
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Reading TSLasso Results}
	\textbf{Interpreting selected functions}
	\begin{itemize}
		\item Functions with large, persistent coefficients across regularization levels:
			\begin{itemize}
				\item Are the main directions of variation,
				\item Effectively parametrize the manifold.
			\end{itemize}
		\item Functions rarely or never selected:
			\begin{itemize}
				\item May correspond to nuisance variation,
				\item Or parameters not actually expressed in the learned representation.
			\end{itemize}
	\end{itemize}
	\vspace{0.7em}
	\textbf{Use in SBI validation}
	\begin{itemize}
		\item Check whether the manifold is primarily parametrized by the \emph{relevant}
			scientific parameters rather than nuisance or noise.
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Subsection: Riemannian Relaxation
% ----------------------------------------------------------------------
\begin{frame}{Step 8c: Manifold Interpretation \& Visualization III: \rrelex \citep{MeilaRRelax}}
	\textbf{Motivation}
	\begin{itemize}
		\item Initial embeddings from \diffmap\ approximate isometries only up to sampling and noise.
		\item Local distortions can complicate interpretation and downstream analysis.
	\end{itemize}
	\vspace{0.7em}
	\textbf{Riemannian Relaxation}
	\begin{itemize}
		\item Start from an initial embedding $\Phi^{indep}$ (e.g.\ after \ies).
		\item Define a distortion loss based on local metric mismatch.
		\item Iteratively adjust $\Phi^{indep}$ via gradient-based optimization to:
			\begin{itemize}
				\item Reduce local distortion,
				\item Move closer to an isometric parametrization of the manifold.
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Riemannian Relaxation: Properties}
	\textbf{Properties}
	\begin{itemize}
		\item Typically the most computationally expensive step in the pipeline.
		\item Optional but useful when:
			\begin{itemize}
				\item Fine geometric accuracy is needed,
				\item Visual interpretability of coordinates is a priority.
			\end{itemize}
		\item Can be run on reduced subsets or with early stopping.
	\end{itemize}
	\vspace{0.7em}
	\textbf{End result}
	\begin{itemize}
		\item A refined embedding that:
			\begin{itemize}
				\item Better preserves distances and angles locally,
				\item Is more faithful to the geometry implied by the encoder.
			\end{itemize}
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Summary
% ----------------------------------------------------------------------
\begin{frame}{Summary: Manifold Interpretation \& Visualization}
	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Preprocessing I: Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Preprocessing II: Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8a.} IES and plotting.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8b.} \tslasso.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8c.} \rrelex.}
			\end{itemize}
		\end{column}
		\begin{column}[t]{0.7\textwidth}
			\textbf{Key takeaways:}
			\begin{itemize}
				\item Three complementary methods help interpret and visualize 
					the manifold, ensuring it captures relevant scientific parameters.
				\item \ies provides independent coordinates for visualization.
				\item \tslasso validates which factors actually parametrize the geometry.
				\item \rrelex (optional) refines embeddings for better interpretability.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}