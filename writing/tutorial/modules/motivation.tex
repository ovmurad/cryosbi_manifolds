% ======================================================================
\section{Motivation}
% ======================================================================

% ----------------------------------------------------------------------
% Brief Motivation
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\begin{frame}{Motivation}
	\begin{itemize}
		\item Examples of relevant SBI (Simulation-Based Inference) problems
		\item Our main Cryo-EM question
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
\begin{frame}{Motivation for Manifold Learning (ML)}
	\begin{itemize}
		\item Many natural data sets are intrinsically low-dimensional
		\item Neural network embeddings are often low-dimensional as well
		\item Therefore: manifold learning methods can help in many ways (incomplete list):
		\begin{itemize}
			\item estimate dimension / detect low dimensional structure
			\item reduce dimension via embedding algorithms
			\item visualize and interpret latent structure
			\item measure/correct distortion of an embedding
			\item check whether simulated data covers real data
			\item perform SBI in a low-dimensional representation
		\end{itemize}
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Datasets
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\begin{frame}{Spectra of galaxies measured by the \green{Sloan Digital Sky Survey (SDSS)}\footnote{Preprocessed by Jacob VanderPlas and Grace Telford}}
	
	\begin{columns}
		\begin{column}{0.3\txtwi}
			\setlength{\picwi}{\textwidth}
			
			\centerline{\href{www.sdss.org}{\gray{\tiny www.sdss.org}}}
			\includegraphics[width=\picwi]{../figures/galaxies/pie_boss_z0-710.jpg}
			
			\centerline{\href{www.sdss.org}{\gray{\tiny www.sdss.org}}}
			\includegraphics[width=\picwi]{../figures/galaxies/specById_asp.png}
		\end{column}
		
		\begin{column}{0.7\txtwi}
			\begin{itemize}
		
				\item $n=675{,}000$ spectra $\times\, D=3750$ dimensions
			\end{itemize}
			
			\includegraphics[width=0.6\txtwi]{../figures/galaxies/pcEmbed_SFR-trim.png}
			\centerline{\gray{\tiny embedding by James McQueen}}
		\end{column}
	\end{columns}
\end{frame}

% ----------------------------------------------------------------------
\begin{frame}{Molecular configurations}
	
	\begin{columns}
		\begin{column}{0.35\txtwi}
			\setlength{\picwi}{\textwidth}
			
			\centerline{\small aspirin molecule}
			\includegraphics[width=\picwi]{../figures/mds/aspirin/aspirin.png}
		\end{column}
		
		\begin{column}{0.7\txtwi}
			
			\begin{itemize}
				\item Data from \green{Molecular Dynamics (MD)} simulations of small molecules \citep{ChmielaMDS}
				\item $n \approx 200{,}000$ configurations $\times\, D \sim 20$â€“$60$
			\end{itemize}
			\includegraphics[width=0.5\txtwi]{../figures/mds/aspirin/aspirin-cc.png}
			
		\end{column}
	\end{columns}
	
\end{frame}

% ----------------------------------------------------------------------
% Why and When to do Manifold Learning
% ----------------------------------------------------------------------
\begin{frame}{When to do (non-linear) dimension reduction}
	
	\setlength{\picwi}{0.3\txtwi}
	% \centerline{\includegraphics[width=\picwi]{IsomapFaces.png}}
	
	\begin{itemize}
		\item high-dimensional \blue{data $p \in \R[D],\; D=64\times 64$}
		\item can be described by a small number $d$ of continuous parameters
		\item Usually, large sample size \blue{$n$}
	\end{itemize}
	
\end{frame}

% ----------------------------------------------------------------------
\begin{frame}{When to do (non-linear) dimension reduction}
	\begin{center}
		\setlength{\picwi}{0.2\txtwi}
		\begin{tabular}{ccc}
			{\tiny HR diagram} & {\tiny aspirin MD simulation} & {\tiny SDSS galaxy spectra} \\
			% \includegraphics[width=0.7\picwi]{HRDiagram.png} &
			\includegraphics[width=\picwi]{../figures/mds/aspirin/aspirin-cc.png} &
			\includegraphics[width=\picwi]{../figures/galaxies/pcEmbed_SFR-trim.png} \\
		\end{tabular}
	\end{center}
	
	\only<1>{
		\begin{itemize}
			\item high-dimensional
			\item can be described by a small number $d$ of continuous parameters
			\item Usually, large sample size \blue{$n$}
		\end{itemize}
	}
	
	\only<2>{
		Why?
		\begin{itemize}
			\item To save space and computation
			\begin{itemize}
				\item $n\times D$ data matrix $\rightarrow n\times m$, $m\ll D$
			\end{itemize}
			\item To use it afterwards in (\green{prediction}) tasks
			\item To \green{understand} the data better
			\begin{itemize}
				\item preserve large-scale features, suppress fine-scale features
			\end{itemize}
		\end{itemize}
	}
\end{frame}

% ----------------------------------------------------------------------
\begin{frame}{Manifold Learning (ML) for the physical sciences}
	\begin{itemize}
		\item big, high-dimensional data
		\item data, physics supports manifold models
		\item understanding \& prediction equally important
		\pause
	\end{itemize}
	
	\red{Challenges} for ML algorithms
	\begin{itemize}
		\item scalable \megaman
		\pause
		ML package \citep{McQueenMegaman} \checkmark
		\pause
		\item \green{find ``something new, trustworthy, reproducible, interpretable''}
		\pause
		\item remove algorithmic artifacts \green{(replace grad student)}.
		\item data-driven parameter selection \green{(replace grad student)}.
		\item validation on mathematical/statistical grounds as much as possible \green{(replace experimental validation)}
		\item use \red{domain knowledge} \green{(not domain expert)}.
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Intro into Algos
% ----------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{frame}{Brief intro to manifold learning algorithms}
	
	\setlength{\picwi}{0.3\txtwi}
	
	\begin{itemize}
		\item \textbf{Input:} data $p_1,\dots,p_n$, embedding dimension \blue{$m$}, neighborhood scale \blue{$\eps$}
		
		\only<2->{%
			\item \green{Construct neighborhood graph:}
			$p,p'$ are neighbors iff {\small $\norm[p-p']^2 \le \eps$} \hfill{\green{(ALL ALGORITHMS)}}
		}

	\end{itemize}
	
	\vspace{-0.4em}
	\only<1-3>{%
		\includegraphics[width=.7\picwi,height=.5\picwi,viewport=100 100 485 360,clip]{../figures/hourglass/pretty-hourglass-sample-n2000.png}%
	}
	\hspace{1em}
	\only<2-3>{%
		\includegraphics[width=.8\picwi]{../figures/hourglass/hourglass2D-n500-snoise0_001-graph-eps0_3-tol0_25.png}%
	}
	\hspace{1em}
	\only<3>{%
		\includegraphics[width=.7\picwi]{../figures/hourglass/pretty-hourglass-laplacian.png}%
	}\\[-0.2em]
	
	\only<1-3>{$p_1,\dots ,p_n \subset \R[D]$}

	\only<4>{%
\begin{itemize}
		\item  Construct a \red{$n\times n$ matrix} whose leading eigenvectors are the \blue{coordinates $\phi(p_{1:n})$} (e-vector based algorithms)
                \item Improve initial coordinates by \green{Attraction-Repulsion} based algorithms
                  \end{itemize}
		}

        
	\only<5>{%

	  \vspace{0.6em}
		\begin{itemize}
			\item \lapmap \citep{BelkinLEM}/ \diffmap \citep{CoifmanDM}
			
			\item Construct similarity matrix
			\[
			S = [S_{pp'}]_{p,p'\in\X}
			\quad\text{with}\quad
			S_{pp'} = \exp[-\frac{\norm[p-p']^2}{\eps}]
			\quad\text{iff $p,p'$ are neighbors.}
			\]
			
			\item Construct \red{Laplacian matrix}
			\(
			L = I - T^{-1}S,
			\qquad
			T = \diag[S\mathbf{1}]
			\)
			
			\item Compute $\phi^{1\dots m} =$ eigenvectors of $L$ (smallest eigenvalues)
			
			\item Coordinates of $p \in \X$ are $(\phi^1(p),\dots,\phi^m(p))$
		\end{itemize}
	}
	
	\only<6>{%
		\vspace{0.6em}
		\begin{itemize}
			\item \isomap \citep{TenenbaumISOMAP}
			
			\item Find all shortest paths in neighborhood graph, construct \red{distance matrix}
			\[
			M_{pp'} = \big[\dist[p, p']\big]
			\]
			
			\item Apply \blue{Multi-Dimensional Scaling (MDS)} to obtain $m$-dimensional coordinates
		\end{itemize}
	}
	
\end{frame}
%------------------------------------------------------------------------
\subsection{Attraction-Repulsion based algorithms}
%------------------------------------------------------------------------
\begin{frame}\frametitle{Attraction-repulsion-based (heuristic) algorithms}
%\begin{frame}<beamer:0|handout:0>\frametitle{Repulsion-based (heuristic) algorithms}
  
\begin{block}{t-Stochastic Neighbor Embedding (t-SNE)}
  \begin{enumerate}
\item[Input] similarity matrix  $S$, embedding dimension $s$
\item[Init] choose embedding points $y_{1:n}\in\rrr^s$ at random
\item\label{step:q} $S_{ii}\gets 0$, normalize rows $d_i=\sum_j S_{ij}$, $P_{ij}=S_{ij}/d_i$
\item symmetrize $P=\frac{1}{2n}(P+P^T)$ \green{$P$ is distribution over pairs of neighbors $(i,j)$}
\item  $\tilde{S}_{ij}=\tilde{\kappa}(\|y_i-y_j\|)$\green{compute similarity in output space}
\item[]where $\tilde{\kappa}(z)=\frac{1}{1+z^2}$ the \green{Cauchy (Student t with 1 degree of freedom)}
\item Define distribution $Q$ with $Q_{ij}\propto S_{ij}$
\item Change $y_{i:n}$ to decrease the \red{Kullbach-Leibler divergence} $KL(P||Q)=\sum_{i,j}P_{ij}\ln \frac{P_{ij}}{Q_{ij}}$ (by gradient descent) and repeat from step \ref{step:q}
  \end{enumerate}
\end{block}

\pause
\begin{itemize}
\item empirically useful for visualizing clusters (repulsion encourages cluster formation)
\item non-deterministic, more parameters
\end{itemize}
 
\end{frame}
%------------------------------------------------------------------------
%\begin{frame}<beamer:0|handout:0>\frametitle{UMAP: Uniform Manifold Approximation and Projection}
\begin{frame}\frametitle{UMAP: Uniform Manifold Approximation and Projection \cite{McInnes, Healy, Melville,2018}}
  \setlength{\picwi}{0.25\textwidth}
\begin{block}{UMAP Algorithm}
\begin{enumerate}
\item[]{\bf Input} $k$ number nearest neighbors, $d$, 
\item Find $k$-nearest neighbors
\item Construct (asymmetric) similarities $w_{ij}$, so that $\sum_jw_{ij}=\log_2 k$. $W=[w_{ij}]$.
\end{enumerate}
\end{block}

%\begin{columns}
%\begin{column}{0.75\txtwi}
\vspace{-3.2em}
\begin{minipage}{0.75\txtwi}
  \begin{block}{}
    \begin{enumerate}
      \setcounter{enumi}{2}
\item Similarity matrix $S=W+W^T-W.*W^T$  
\item \green{Initialize embedding $\phi$ by {\sc LaplacianEigenmaps}}.
\item Optimize embedding.
 \item[] Iteratively for $n_{iter}$ steps
 \begin{enumerate}
 \item Sample an edge $ij$ with probability $\propto \exp{-d_{ij}}$
 \item Move $\phi_i$ towards $\phi_j$
 \item Sample a random $j'$ uniformly
 \item Move $\phi_i$ away from $\phi_{j'}$
   \end{enumerate}
% \item[]\green{\small Stochastic approximate \red{logistic regression} of $||\phi_i-\phi_j||$ on $d_{ij}$.}
\item[]{\bf Output} $\phi$
 \end{enumerate}
  \end{block}
  \end{minipage}
%\end{column}
%\begin{column}{0.2\txtwi}
\hfill\raisebox{-4em}{\includegraphics[width=\picwi]{Figures-aspirin/ethanol_umap3D.png}}
%\end{column}
%\end{columns}
\end{frame}

%------------------------------------------------------------------------
\begin{frame}{Comparison between algorithms}
  %\begin{frame}<beamer:0|hadout:0>{\isomap vs. \diffmap}
  {\isomap vs. \diffmap}
	
	\begin{columns}
		\begin{column}{0.5\txtwi}
			% \includegraphics[
			% trim=1.5cm 1.5cm 1.5cm 1.5cm,
			% clip,
			% width=0.46\textwidth
			% ]{PARISDominique/Figures/IsomapFaces.png}
			
			\isomap
			\begin{itemize}
				\item Preserves geodesic distances
				\begin{itemize}
					\item but only sometimes \citep{TODO}
				\end{itemize}
				\item All-pairs shortest paths \blue{$\mathcal{O}(n^3)$}
				\item Stores/processes a \green{dense} matrix
			\end{itemize}
		\end{column}
		
		\begin{column}{0.5\txtwi}
			% \includegraphics[
			% trim=1.5cm 1.5cm 1.5cm 1.5cm,
			% clip,
			% width=0.46\textwidth
			% ]{PARISDominique/Figures/EigenmapFaces.png}
			
			\diffmap
			\begin{itemize}
				\item Distorts geodesic distances
				\item Uses only nearest-neighbor distances \blue{$\mathcal{O}(n^{1+\delta})$} (sparse graph)
				\item Stores/processes a \green{sparse} matrix
			\end{itemize}
		\end{column}
	\end{columns}

Attraction-Repulsion  based algorithms (\tsne vs. \umap)
	\begin{itemize}
		\item \tsne, \umap: visualization-first, heuristic
                \item \tsne with appropriate choice of parameters can emulate \umap \cite{B\"ohm et al., 2022}
        \end{itemize}

        Eigenvector-based vs. Attraction-Repulsion based
        \begin{itemize}
        \item Eigenvector-based: subject to artefacts: ``horse-shoes'', collapsed embeddings
        \item Repulsion: always ``fills space'' 
          \end{itemize}
      
\end{frame}









%------------------------------------------------------------------------
% Toy Example
%------------------------------------------------------------------------
%------------------------------------------------------------------------
\begin{frame}{A Toy Example}
	
	A toy example (the ``Swiss Roll'' with a hole)
	
	\vspace{2em}
	\setlength{\picwi}{0.55\txtwi}
	
	\begin{tabular}{cc}
		points in $D \ge 3$ dimensions & same points reparametrized in 2D \\
		% \includegraphics[width=0.8\picwi]{Figures/mani-swisshole-1000-k8-isomap-6-data.png}
		&
		% \hspace{-2.5em}\includegraphics[width=1.2\picwi,height=0.3\picwi]{Figures/mani-swisshole-1000-k8-ltsa-6-embe.png}
		\\
		\green{\textbf{Input}} & \green{\textbf{Desired output}}
	\end{tabular}
\end{frame}

%------------------------------------------------------------------------
\begin{frame}{Embedding in 2 dimensions by different manifold learning algorithms}
	
	\hspace{5.7em}\green{\textbf{Input}} \\[-0.3em]
	
	\setlength{\picwi}{0.9\txtwi}
	\begin{center}
		% \includegraphics[width=\picwi]{Figures/fig-mani-swisshole.png}\\
		\gray{\tiny Figure by Todd Wittman}
	\end{center}
	
\end{frame}

%------------------------------------------------------------------------
\begin{frame}{Embedding in 2 dimensions by different manifold learning algorithms}
	
	\setlength{\picwi}{0.3\txtwi}
	
	\begin{small}
		\begin{columns}
			
			\begin{column}{\picwi}
				Original data\\
				{\small (Swiss Roll with hole)}\\
				% \includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-isomap-6-data.png}\\
				{\small \hessmap (\hessmap*)}\\
				% \includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k12-he-6-embe.png}
			\end{column}
			
			\begin{column}{\picwi}
				{\small \lapmap (\lapmap*)}\\
				% \includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-le-embe.png}\\
				{\small \lle (\lle*)}\\
				% \includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-lle-6-embe.png}
			\end{column}
			
			\begin{column}{\picwi}
				{\small \isomap}\\
				% \includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-isomap-6-embe.png}\\
				{\small \ltsa (\ltsa*)}\\
				% \includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-ltsa-6-embe.png}
			\end{column}
			
		\end{columns}
	\end{small}
	
\end{frame}

%------------------------------------------------------------------------
% Evaluation
%------------------------------------------------------------------------
% ----------------------------------------------------------------------
\begin{frame}{How to evaluate the results objectively?}
	
	\begin{itemize}
		\item Many algorithms exist:
		\blue{\isomap, \lapmap (\lapmap*), \diffmap (\diffmap*), \hessmap (\hessmap*), \lle (\lle*), \ltsa (\ltsa*)}
		\item Each algorithm gives a different embedding of the same data
	\end{itemize}
	
	\setlength{\picwi}{0.15\txtwi}
	
	\begin{small}
		\begin{columns}
			
			\begin{column}{\picwi}
				{\tiny Original} \\
				%\includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-isomap-6-data.png}\\
				{\tiny \hessmap*}\\
				%\includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k12-he-6-embe.png}
			\end{column}
			
			\begin{column}{\picwi}
				{\tiny \lapmap*}\\
				%\includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-le-embe.png}\\
				{\tiny \lle*}\\
				%\includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-lle-6-embe.png}
			\end{column}
			
			\begin{column}{\picwi}
				{\tiny \isomap*}\\
				%\includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-isomap-6-embe.png}\\
				{\tiny \ltsa*}\\
				%\includegraphics[width=\picwi]{Figures/mani-swisshole-1000-k8-ltsa-6-embe.png}
			\end{column}
			
			\begin{column}{0.7\txtwi}
				\begin{itemize}
					\item which of these embeddings are ``correct''?
					\item if several are ``correct'', how do we reconcile them?
					\item if not ``correct'', what failed?
				\end{itemize}
			\end{column}
			
		\end{columns}
	\end{small}
	
\end{frame}

%------------------------------------------------------------------------
\begin{frame}{How to evaluate the results objectively?}
	
	\setlength{\picwi}{0.45\txtwi}
	
	\begin{small}
		\begin{columns}
			
			\begin{column}{\picwi}
				\only<1>{
					% \includegraphics[width=\picwi]{Figures/fig-mani-swisshole.png}
				}
				\only<2>{%
					% \includegraphics[width=\picwi]{Figures/jvdp-spectra-data.png} \\
					\gray{\tiny Spectrum of a galaxy. Source SDSS, Jake VanderPlas}
				}
			\end{column}
			
			\begin{column}{0.7\txtwi}
				\begin{itemize}
					\item which of these embeddings are ``correct''?
					\item if several are ``correct'', how do we reconcile them?
					\item if not ``correct'', what failed?
					\only<2>{\item what if I have real data?}
				\end{itemize}
			\end{column}
			
		\end{columns}
	\end{small}
	
	\only<1>{\tiny
		Algorithms: \blue{\mds* , \pca* , \isomap*, \lle* , \hessmap* , \lapmap* , \diffmap*}
	}
	
\end{frame}
