%\begin{frame}\frametitle{PCA -- Linear dimension reduction}
%\end{frame}
%------------------------------------------------------------------------
\begin{frame}\frametitle{A toy example (the ``Swiss Roll'' with a hole)}
\setlength{\picwi}{0.5\tttwi}
\begin{columns}
\begin{column}{0.4\tttwi}
  \centerline{\mydef{Input}}
  points in $\dhigh\geq 3$ dimensions 
  \includegraphics[width=0.8\picwi]{Figures/mani-swisshole-1000-k8-isomap-6-data.png}
\end{column}
\begin{column}{0.6\tttwi}
  \centerline{\mydef{Desired output}}
    same points reparametrized in 2D 
\includegraphics[width=1.2\picwi,height=0.3\picwi]{Figures/mani-swisshole-1000-k8-ltsa-6-embe.png}
\pause
Linear dimension reduction fails\\
\includegraphics[width=0.4\picwi,height=0.6\picwi]{Figures/fig-mani-swisshole-pcacut.png}
\hspace{1em}\includegraphics[width=0.4\picwi,height=0.6\picwi]{Figures/fig-mani-swisshole-mdscut.png}
\end{column}
\end{columns}

\end{frame}
%------------------------------------------------------------------------
\begin{frame}<beamer:0|handout:0>\frametitle{Neighborhood graphs}
%\begin{frame}\frametitle{Neighborhood graphs}

\bit
\item All ML algorithms start with a \mydef{neighborhood graph} over the data points
  \bit
\item  $\neigh_i$ denotes the neighbors of $\xi_i$, and $k_i=|\neigh_i|$. 
\item  $\Xi_i=[\xi_{i'}]_{i'\in\neigh_i}\in \rrr^{\dhigh\times k_i}$ contains the coordinates of $\xi_i$'s neighbors
  \eit
\item In the \mydef{radius-neighbor} graph, the neighbors of $\xi_i$ are the points within distance $r$ from $\xi_i$, i.e. in the ball $B_r(\xi_i)$.
\item In the \mydef{k-nearest-neighbor (k-nn)} graph, they are the $k$ nearest-neighbors of $\xi_i$. 


\item[]
\item k-nn graph has many computational advantages
  \bit
\item constant degree $k$ (or $k-1$) 
\item connected for any $k>1$
\item more software available
\item[]
  \item but much more difficult to use for \myemph{consistent} estimation of manifolds (see later, and \cite{}) 
    \eit
   \eit
   \setlength{\picwi}{0.3\textwidth}

   \begin{tabular}{ccc}
\includegraphics[width=.7\picwi,height=.5\picwi,viewport=100 100 485 360,clip]{pretty-hourglass-sample-n2000.png}&
\hspace{1em}
\includegraphics[width=.8\picwi]{fig_hourglass2D-n500-snoise0_001-graph-eps0_3-tol0_25.png}&
\hspace{1em}
\includegraphics[width=.7\picwi]{pretty-hourglass-sample-n2000.png}&
\hspace{1em}
\includegraphics[width=.8\picwi]{fig_hourglass2D-n500-snoise0_001-graph-eps0_3-tol0_25.png}&
\hspace{1em}
\includegraphics[width=.7\picwi]{pretty-hourglass-laplacian.png}\\
data $\xi_1,\ldots \xi_n\,\subset\,\rrr^D$
&
neighborhood graph
&
$A$ (sparse) matrix of\\
&&distances between neighbors\\
\end{tabular}
\end{frame}
%------------------------------------------------------------------------
\subsection{E-vector based embedding algorithms}
%------------------------------------------------------------------------
\begin{frame}<beamer:0|handout:0>\frametitle{Embedding algorithms}
%\begin{frame}\frametitle{Embedding algorithms}

Diffusion Maps/Laplacian Eigenmaps, Isomap, LTSA, MVU, Hessian Eigenmaps, SketchMap \likecite{}\ldots
  
\bit
\item Map $\dataset$ to $\rrr^m$ where $m\geq d$ (global coordinates)
\item Can also map a local neighborhood $U\subseteq \dataset$ to $\rrr^d$ (local, intrinsic coordinates)
 \item[]
  \item[]\mydef{Input}
  \item   embedding dimension $m$
  \item  neighborhood radius/kernel width {$\epsilon$}
    \bit
  \item usually radius $r\approx 3\times \epsilon$ 
    \eit
  \item  neighborhood graph
    
  \item[] $\{\neigh_i,\,\Xi_i,\text{ for }i=1:n\}$
  \item[] $A=[\|\xi_i-\xi_j\|]_{i,j=1}^n$ distance matrix, with $A_{ij}=\infty$ if $i\not\in\neigh_j$ 
\eit

\end{frame}

%------------------------------------------------------------------------
%\begin{frame}\frametitle{The Isomap algorithm}
\begin{frame}<beamer:0|handout:0>\frametitle{The Isomap algorithm}
\begin{block}{Isomap Algorithm \likecite{Tennenbaum, deSilva \& Langford 00}}
  \benum
\item[]{\bf Input} $A$, dimension $d$ 
\item Find all shortest path distances in neighborhood graph
\item[] if $A_{ij}=\infty$, then $A_{ij}\,\gets$ graph distance between $i,j$
\item Construct \myred{matrix of squared distances}
\[M=[(A_{ij})^2]\]
  \pause
\item use \myblue{Multi-Dimensional Scaling}  MDS$(M,d)$ to obtain $d$ dimensional coordinates $Y$ for $\dataset$
\eenum
\end{block}

\bit
\item Works also for $m>d$
\eit
\end{frame}
%------------------------------------------------------------------------

\begin{frame}\frametitle{The Diffusion Maps (DM)/ Laplacian Eigenmaps (LE) Algorithm}
\begin{block}{Diffusion Maps Algorithm}
\benum
\item[] {\bf Input} distance matrix $A\in \rrr^{n\times n}$ , bandwidth $\epsilon$, embedding dimension $m$
\item Compute Laplacian $L\in \rrr^{n\times n}$
\item Compute eigenvectors of $L$ for \myemph{smallest $m+1$ eigenvalues} $[\phi_0\,\phi_1\,\ldots \phi_m]\in\rrr^{n\times m}$
  \bit
  \item $\phi_0$ is constant and not informative
%  \item These are the \myemph{slow modes} of the system
  \eit
\item[] The \mydef{embedding coordinates} of $p_{i}$ are $(\phi_{i1},\ldots
  \phi_{im})$
\eenum
\end{block}
\setlength{\picwi}{0.3\textwidth}
%\hfill\includegraphics[width=\picwi]{Figures-aspirin/ethanol-tau10-noaxes.png}
\end{frame}
%------------------------------------------------------------------------
%\begin{frame}\frametitle{The (renormalized) Laplacian}
\begin{frame}<beamer:0|handout:0>\frametitle{The Laplacian}
\begin{block}{Laplacian}
\benum
\item[] Input distance matris $A\in\rrr^{n\times n}$, \mydef{bandwidth} ${\epsilon}$
\item Compute \mydef{similarity matrix} $S_{ij}=\exp\left(-\frac{A_{ij}^2}{\epsilon^2}\right)=\kappa(A_{ij}/\epsilon)$
  \pause
\item Normalize columns $\quad\quad d_j=\sum_{i=1}^n S_{ij}$, $\quad\tilde{L}_{ij}=S_{ij}/{d_j}$
\item Normalize rows $\quad\quad\quad d'_i=\sum_{j=1}^n \tilde{L}_{ij}$, $\quad P_{ij}=\tilde{L}_{ij}/d_i'$
  \pause
\item $L=\frac{1}{\epsilon^2}(I-P)$
\item{Output} $L$, $d'_{i}/d_i$
\eenum
\end{block}
\pause
\bit
\item Laplacian $L$ central to understanding the manifold geometry
\item $\lim_{n\rightarrow \infty}L\,=\,\Delta_\M$ \likecite{Coifman,Lafon 2006} 
\item Renormalization trick cancels effects of (non-uniform) sampling density \likecite{Coifman,Lafon 2006}  
\item[]
 \pause
\item[] Other Laplacians
\item $L^{un}=\diag\{d_{1:n}\}-A$ \hfill\myemph{unnormalized} Laplacian
\item $L^{rw}=I-\diag\{d_{1:n}\}^{-1}A$ \hfill\myemph{random walk} Laplacian
\item $L^{n}=I-\diag\{d_{1:n}\}^{-1/2}A\diag\{d_{1:n}\}^{-1/2}$ \hfill\myemph{normalized} Laplacian
  \eit
\note{\begin{myproblem}{Renormalized Laplacian}
{\bf a.} Show that $L\bbone{}=0$ for the renormalized Laplacian. Hence $L$ always has a 0 e-value. 
  \end{myproblem}
%-------------------------------------
  \begin{myproblem}[Unnormalized Laplacian]
  Let $L^{un}=D-A$ be the {\em unnormalized Laplacian} of graph defined by $A$. Prove that $x^TL^{un}x=\sum_{(i,j)\in{\cal E}} (x_i-x_j)^2$ for any $x\in\rrr^n$.
  \end{myproblem}
  %-------------------------------------
  \begin{myproblem}[Double Normalization Laplacian]
A more standard presentation of the Re-normalized Laplacian is this:
\benum
\item Compute similarity matrix $S$
\item First normalization $d_i=\sum_{j=1}^n S_{ij}$, $\tilde{L}_{ij}=S_{ij}/{d_id_j}$ (symmetric matrix)
\item Second normalization $d'_i=\sum_{j=1}^n \tilde{L}_{ij}$, $P_{ij}=\tilde{L}_{ij}/d_i'$ (asymmetric)
\item $L=\frac{1}{\epsilon^2}(I-P)$
  \eenum
 Show that this $L$ is the same as in the algorithm on the previous page.
    \end{myproblem}
%-------------------------------------
}% end note
\end{frame}
%------------------------------------------------------------------------
\begin{frame}<beamer:0|handout:0>\frametitle{Isomap vs. Diffusion Maps}
%\begin{frame}\frametitle{Isomap vs. Diffusion Maps}

\begin{columns}
\begin{column}{0.5\tttwi}
\includegraphics[trim= 1.5cm 1.5cm 1.5cm 1.5cm,clip=true,width=0.46\textwidth]{PARISDominique/Figures/IsomapFaces.png}  

\mydef{Isomap}
\bit
\item Preserves geodesic distances
  \bit
  \item but only when $\M$ is \myemph{flat} and ``data'' convex
  \eit
\item Computes all-pairs shortest paths $\bigOO(n^3)$
\item Stores/processes \myemph{dense} matrix 
\eit
\end{column}
\begin{column}{0.5\tttwi}
\includegraphics[trim= 1.5cm 1.5cm 1.5cm 1.5cm,clip=true,width=0.46\textwidth]{PARISDominique/Figures/EigenmapFaces.png} 

\mydef{DiffusionMap}
\bit
\item Distorts geodesic distances
\item Computes only distances to nearest neighbors $\bigOO(n^{1+\epsilon})$  % or sqrt{\epsilon}??
\item Stores/processes \myemph{sparse} matrix 
\eit
\end{column}
\end{columns}
\vfill
\bit
\item t-SNE, UMAP visualization algorithms
\eit
\end{frame}
%\end{document} %%%%%%%%%%%%%%
%------------------------------------------------------------------------
\subsection{Repulsion-based algorithms}
%------------------------------------------------------------------------
%\begin{frame}\frametitle{Repulsion-based (heuristic) algorithms}
\begin{frame}<beamer:0|handout:0>\frametitle{Repulsion-based (heuristic) algorithms}
  
\begin{block}{t-Stochastic Neighbor Embedding (t-SNE)}
  \benum
\item[Input] similarity matrix  $S$, embedding dimension $s$
\item[Init] choose embedding points $y_{1:n}\in\rrr^s$ at random
\item\label{step:q} $S_{ii}\gets 0$, normalize rows $d_i=\sum_j S_{ij}$, $P_{ij}=S_{ij}/d_i$
\item symmetrize $P=\frac{1}{2n}(P+P^T)$ \myemph{$P$ is distribution over pairs of neighbors $(i,j)$}
\item  $\tilde{S}_{ij}=\tilde{\kappa}(\|y_i-y_j\|)$\myemph{compute similarity in output space}
\item[]where $\tilde{\kappa}(z)=\frac{1}{1+z^2}$ the \myemph{Cauchy (Student t with 1 degree of freedom)}
\item Define distribution $Q$ with $Q_{ij}\propto S_{ij}$
\item Change $y_{i:n}$ to decrease the \mydef{Kullbach-Leibler divergence} $KL(P||Q)=\sum_{i,j}P_{ij}\ln \frac{P_{ij}}{Q_{ij}}$ (by gradient descent) and repeat from step \ref{step:q}
  \eenum
\end{block}

\pause
\bit
\item empirically useful for visualizing clusters (repulsion encourages cluster formation)
\item non-deterministic, more parameters
\eit
 
\end{frame}
%------------------------------------------------------------------------
\begin{frame}<beamer:0|handout:0>\frametitle{UMAP: Uniform Manifold Approximation and Projection}
%\begin{frame}\frametitle{UMAP: Uniform Manifold Approximation and Projection \likecite{McInnes, Healy, Melville,2018}}
  \setlength{\picwi}{0.25\textwidth}
\begin{columns}
\begin{column}{0.75\tttwi}
\begin{block}{UMAP Algorithm}
\benum
\item[]{\bf Input} $k$ number nearest neighbors, $d$, 
\item Find $k$-nearest neighbors
\item Construct (asymmetric) similarities $w_{ij}$, so that $\sum_jw_{ij}=\log_2 k$. $W=[w_{ij}]$.
\item Similarity matrix $S=W+W^T-W.*W^T$  
\item \myemph{Initialize embedding $\phi$ by {\sc LaplacianEigenmaps}}.
\item Optimize embedding.
 \item[] Iteratively for $n_{iter}$ steps
 \benum
 \item Sample an edge $ij$ with probability $\propto \exp{-d_{ij}}$
 \item Move $\phi_i$ towards $\phi_j$
 \item Sample a random $j'$ uniformly
 \item Move $\phi_i$ away from $\phi_{j'}$
% \item[]\myemph{\small Stochastic approximate \mydef{logistic regression} of $||\phi_i-\phi_j||$ on $d_{ij}$.}
\eenum
\item[]{\bf Output} $\phi$
  \eenum
  \end{block}
\end{column}
\begin{column}{0.2\tttwi}
\hfill\includegraphics[width=\picwi]{Figures-aspirin/ethanol_umap3D.png}\end{column}
\end{columns}

\vfill
\pause
\bit
\item t-SNE with appropriate choice of parameters can emulate UMAP \likecite{B\"ohm et al., 2022}
\eit

\end{frame}
%------------------------------------------------------------------------
\begin{frame}\frametitle{Embedding algorithms summary}
\begin{columns}
\begin{column}{0.5\tttwi}
  \myred{E-vector based} % minimize a functional
  \bit
\item \mydef{DiffusionMaps}
\item Isomap
\item LTSA \likecite{Zhang, Zha, 2004}
\item \ldots
\item[$+$] well studied, params better understood
\item[$-$] collapsed embeddings, \myemph{``horseshoes''} possible
%%% *** *** \item require embedding dimension $s\geq d$
  \eit
\end{column}
\pause
\begin{column}{0.5\tttwi}
  \myred{Repulsion based}
  \bit
\item t-SNE
\item UMAP
\item \ldots
\item[]
\item[$-$] heuristic, more parameters
\item[$+$] no collapsed embeddings
\item[]
%%% *** *** \item  $s=d$ intrinsic dimension
  \eit
\end{column}
\end{columns}

\pause
\vspace{2em}
  \bit
%\item All algorithms start from neighborhood graph + distance matrix $A$ (or similarity $S$)
%\item E-vector or repulsion based
\item  Embeddings are sensitive to
  \bit
\item neighborhoods scale \myblue{$\epsilon$} and type  \myblue{K-nn vs. spherical}
\item data non-uniformity 
\item aspect ratio (E-vector based)
  \eit
  \pause
\item Almost always embedding algorithms \myemph{distort} shape of data
  \eit
  
\end{frame}
%------------------------------------------------------------------------



