% ======================================================================
\section{SBI Validation Pipeline}
% ======================================================================
% ----------------------------------------------------------------------
% Overview: Evaluate Coverage and Fidelity
% ----------------------------------------------------------------------
\begin{frame}[t]{SBI Validation Pipeline: Evaluate Coverage and Fidelity}

\footnotesize

\begin{columns}[t]
	
	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Shared Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
		\end{itemize}
	\end{column}

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Specific Step}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cPurple!15}{\textbf{Step 4.} Assess Coverage and Fidelity.}
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Overview: Intrinsic Dimensionality Estimation
% ----------------------------------------------------------------------
\begin{frame}[t]{SBI Validation Pipeline: Intrinsic Dimensionality Estimation}

\footnotesize

\begin{columns}[t]

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Shared Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 5.} Uniform Resampling.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
		\end{itemize}
	\end{column}

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Specific Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cGreen!15}{\textbf{Step 8.} Recompute Local Statistics.}
			\item[]
			\stepbox{cGreen!15}{\textbf{Step 9.} Estimate intrinsic dimension $d$.}
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Overview: Manifold Interpretation and Visualization
% ----------------------------------------------------------------------
\begin{frame}[t]{SBI Validation Pipeline: Manifold Interpretation \& Visualization}

\footnotesize

\begin{columns}[t]

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Shared Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
			\item[]
			\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 5.} Uniform Resampling.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
			\item[]
			\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
		\end{itemize}
	\end{column}

	\begin{column}[t]{0.5\textwidth}
		\begin{center}
			\textbf{Specific Steps}
		\end{center}
		\begin{itemize}
			\setlength\itemsep{2pt}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 8.} Selecting Independent Coordinates: \ies*.}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 9.} Identifying Manifold Parametrization I: Visualization.}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 10.} Identifying Manifold Parametrization II: \tslasso.}
			\item[]
			\stepbox{cOrange!15}{\textbf{Step 11.} Improving Visualization: \rrelax.}
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

% ======================================================================
% Evaluate Coverage and Fidelity
% ======================================================================
% ----------------------------------------------------------------------
% Overview
% ----------------------------------------------------------------------
% TODO: Could add a schematic showing coverage gaps vs good coverage
% TODO: Fidelity assessment is not covered in the paper, and might be something to remove for now, but could be a future direction.
% TODO: If we do end up talking about fidelity, should add picture of coverage and fidelity visualizations.
\begin{frame}{Evaluate Coverage and Fidelity: Overview}

	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
				\item[]
				\stepbox{cPurple!15}{\textbf{Step 4.} Assess Coverage and Fidelity.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Goal:} Compare distributions $p_s$(simulation) vs. $p_e$(experimental) by assessing:
			\begin{itemize}
				\item \textbf{Coverage}: Whether $p_s$ "covers" $p_e$(i.e. simulation generalizes to experimental data).
				\item \textbf{Fidelity}: Whether $p_e$ "covers" $p_s$(i.e. simulation produces realistic data).
			\end{itemize}

			\textbf{Why this matters for SBI:} If $p_s$ and $p_e$ are not similar, then:
			\begin{itemize}
				\item SBI posteriors trained on data from $p_s$ may be mis-calibrated.
				\item Simulator might need refinement.
			\end{itemize}
		\end{column}

	\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Step 1: Distance Matrix and Local Statistics
% ----------------------------------------------------------------------
\begin{frame}{Step 1: Construct Distance Matrix $\distmat$}

	\textbf{Optional: Sub-sampling for tractability}
	\begin{itemize}
        \small
		\item Many downstream operations scale super-linearly in $\N$, the number of data points:
        \item[] \begin{itemize}
            \item Pairwise distances / neighbor graphs,
            \item Spectral decompositions of graph Laplacians,
            \item Local statistics(distance to $k$-nearest neighbors, number of neighbors within radius $r$).
        \end{itemize}
		\item If $\N$ is very large, randomly sub-sample to a size that:
        \item[] \begin{itemize}
            \item Still captures the geometry of the data, % TODO: Could this be clarified?
            \item Fits within the available computational budget.
        \end{itemize}
		\item Use as many points as resources allow.
	\end{itemize}

	\textbf{Core Step: Distance matrix}
	\begin{itemize}
        \small
		\item Compute distances between points in $\X$.
		\item Store distances in sparse form by only storing the distances between nearby points.
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Step 2: Compute Local Statistics
% ----------------------------------------------------------------------
\begin{frame}{Step 2: Compute Local Statistics}

	\textbf{For each point $x \in \X$, compute either:}
	\begin{itemize}
        \small
		\item $\dk[x]$: Distance to the $k$-th nearest neighbor for a range $k \in \{k_{\min}, \dots, k_{\max}\}$.
		\item[]\begin{center} {\LARGE or} \end{center}
		\item $\nr[x]$: The number of neighbors within radius $r$ for a range $r \in \{r_{\min}, \dots, r_{\max}\}$.
	\end{itemize}

	% \textbf{Why the distance matrix is central:}
	% \begin{itemize}
    %     \small
	% 	\item Construction of the affinity matrix $\affmat$ and graph Laplacian $\lapmat$.
	% 	\item Local density estimates via local statistics.
	% 	\item Intrinsic dimensionality estimators via local statistics.
	% 	\item Tangent-space estimation via local covariance.
	% \end{itemize}

	\textbf{Practical notes:}
	\begin{itemize}
        \small
		\item Use approximate nearest neighbor methods for large $\N$.
		\item Reuse the same local statistics across multiple steps in the pipeline.
	\end{itemize}

\end{frame}


% ----------------------------------------------------------------------
% Step 3: Outlier Removal
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
% TODO: Could show before/after outlier removal visualization.
% TODO: Could do an ablation study of embedding methods with and without outlier removal.
% TODO: Could show a plot of mvs volumes for different alphas.
\begin{frame}{Step 3: Outlier Removal}

	\textbf{Motivation}: Highly noisy or pathological points:
    \begin{itemize}
        \small
        \item Distort local geometry,
        \item Destabilize spectral embeddings,
        \item Bias intrinsic dimension estimates.
    \end{itemize}

    \textbf{Goal}: {\small We want a \emph{clean} subset $\X[clean] \subseteq \X$ for all downstream operations.}

	\textbf{Recommended approach - \mvs(\mvs*) \citep{TODO}}:
	\begin{itemize}
        \small
		\item Robust and theoretically grounded.
		\item Uses the same local statistics used later in the pipeline for dimensionality estimation.
	\end{itemize}

    \textbf{But good outlier removal works}: {\small One-Class SVMs, Isolation Forests, DBSCAN, etc.}

\end{frame}

% ----------------------------------------------------------------------
% TODO: Show histogram of density scores and selection of alpha threshold
\begin{frame}{Outlier Removal with \mvs(\mvs*) \citep{TODO}}

	\textbf{How it works theoretically}:
	\begin{itemize}
        \small
		\item For $\alpha \in (0,1)$, define an $\alpha$-\mvs*\ as the smallest set containing at least fraction $\alpha$ of the probability mass.
		\item Flag points outside the $\alpha$-\mvs*\ as outliers and remove them.
	\end{itemize}

	\textbf{In practice, we can estimate an $\alpha$-\mvs*\ with neighbor-based scores}:
	\begin{itemize}
		\small
		\item For each point $x$:
			\begin{itemize}
				\item Compute a relative density score (e.g.\ $1/\dk[x]$ or $\nr[x]$),
				\item Rank all points by this score.
			\end{itemize}
		\item Keep roughly the top $\alpha \N$ highest-density points as in-distribution,
		\item Discard the remaining $(1-\alpha) \N$ as outliers.
		\item Optionally average rankings over multiple $k$ / $r$ for robustness.
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Step 4: Coverage and Fidelity Assessment
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\begin{frame}{Step 4: Assess Coverage and Fidelity}

	\textbf{Goal:} Verify whether $p_s$(simulation) $\approx p_e$(experimental) by assessing:
	\begin{itemize}
		\small
		\item \textbf{Coverage}: whether $p_s$ ``covers'' $p_e$ (i.e.\ simulations generalize to experimental data).
		\item \textbf{Fidelity}: whether $p_e$ ``covers'' $p_s$ (i.e.\ simulations produce realistic data).
	\end{itemize}
	
	\textbf{How it works theoretically:}
	\begin{itemize}
		\small
		\item $D_{\mathrm{KL}}(p_e \parallel p_s)$ evaluates \textbf{coverage}: high values $\Rightarrow$ poor coverage.
		\item $D_{\mathrm{KL}}(p_s \parallel p_e)$ evaluates \textbf{fidelity}: high values $\Rightarrow$ poor fidelity.
	\end{itemize}
	
	\textbf{How it works in practice:}
	\begin{itemize}
		\small
		\item Use training subsets of $\X[s][clean]$ and $\X[e][clean]$ to estimate density models $\hat{p}_e$ and $\hat{p}_s$.
		\item Estimate divergences via Monte Carlo integration using held-out samples.
	\end{itemize}
	
\end{frame}

% ----------------------------------------------------------------------
\begin{frame}{Assessing Coverage and Fidelity with KDEs}

	\textbf{Recommended approach - \kde(\kde*) \citep{TODO}:}
	\begin{itemize}
		\footnotesize
		\item Uses matrices we already have from the preprocessing steps.
		\item Naturally supports Monte Carlo estimation of KL-divergences.
		\item Performance adapts and improves as the intrinsic data dimension decreases.
	\end{itemize}

	\textbf{Density estimation via \kde*:}
	\begin{itemize}
		\footnotesize
		\item For a dataset $\X = \{x_i\}_{i=1}^{\N} \sim p$, kernel function $K_h(\cdot)$, bandwidth $h$, the \kde*\ is given by:
		\[ 
			\hat{p}(x) = \frac{1}{\N} \sum_{i=1}^{\N} K_h(x - x_i) 
		\]
		\item We split $\X[s][clean]$ and $\X[e][clean]$ into training and test sets.
		\item We use the training data to fit models $\hat{p}_e$ and $\hat{p}_s$ and select the bandwidth $h$ via validation.
		\item The test sets are used for Monte Carlo KL estimation:
		\item[] 
			\begin{itemize}
			\footnotesize
			\item \textbf{Coverage} is assessed using
			\(
				D_{\mathrm{KL}}(p_e \parallel p_s)
				\approx
				\frac{1}{|\X[e][test]|}
				\sum_{x_e \in \X[e][test]}
				\Big(
					\log \hat{p}_e(x_e)
					-
					\log \hat{p}_s(x_e)
				\Big)
			\).
			\item \textbf{Fidelity} uses the reverse divergence $D_{\mathrm{KL}}(p_s \parallel p_e)$ which is estimated analogously.
			\end{itemize}
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Summary
% ----------------------------------------------------------------------
\begin{frame}{Summary: Evaluate Coverage and Fidelity}

	\begin{columns}[t]

		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
				\item[]
				\stepbox{cPurple!15}{\textbf{Step 4.} Assess Coverage and Fidelity.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Key takeaways:}
			\begin{itemize}
				\item \textbf{Coverage} and \textbf{Fidelity} formalize mismatch between simulated and experimental.
				\item We distinguish two failure modes:
				\begin{itemize}
					\item lack of \textbf{coverage} (real data lie outside the support of simulations).
					\item lack of \textbf{fidelity} (simulations generate unrealistic samples).
				\end{itemize}
				\item Assessing these properties tests whether the learned representation is suitable for downstream SBI.
			\end{itemize}
		\end{column}

	\end{columns}

\end{frame}

% ======================================================================
% Intrinsic Dimensionality Estimation
% ======================================================================
% ----------------------------------------------------------------------
% Overview
% ----------------------------------------------------------------------
\begin{frame}{Intrinsic Dimensionality Estimation: Overview}

	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cGreen!15}{\textbf{Step 8.} Estimate intrinsic dimension $d$.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			
			\small

			\textbf{Goal:} Estimate the intrinsic dimension $d$ of the simulation and experimental manifolds $\M_s$ and $\M_e$. 
			Use multiple estimators with different parameters to provide a robust estimate of $d$.

			\vspace{0.5em}

			\textbf{Why this matters for SBI:}
			\begin{itemize}
				\item Check if the simulation and experimental data manifolds have similar intrinsic dimensions.
				\item If working with neural network embeddings, check that the network has learned a low-dimensional representation of the data.
				\item Understanding the complexity of the data distribution.
				\item Guiding downstream analysis and visualization.
			\end{itemize}

		\end{column}
	\end{columns}

\end{frame}

% ----------------------------------------------------------------------
% Step 4: Recompute Local Statistics
% ----------------------------------------------------------------------
\begin{frame}{Step 4: Recompute Local Statistics}

	\small
	We use the same steps as in the previous section to remove outliers from the data and obtain the clean subset $\X[clean]$:
	\begin{itemize}
		\setlength\itemsep{1pt}
		\item[]
		\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
		\item[]
		\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
		\item[]
		\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
	\end{itemize}

	\textbf{This ensures that the local statistics reflect the geometry of the clean data and are not biased/distorted by the outliers.}

\end{frame}

% ----------------------------------------------------------------------
% Step 5: Uniform Resampling
% ----------------------------------------------------------------------
% TODO: Could show density distribution before/after resampling or picture from the paper
\begin{frame}{Step 5: Uniform Resampling}

	\small

	\textbf{Goal:} Resample the data to imitate a sample from a uniform distribution over $\M$.

	\textbf{Reasoning:}
	\begin{itemize}
		\item A non-uniform data distribution over $\M$ entangles geometric structure with distribution artifacts. Specifically, non-uniform densities:
		\begin{itemize}
			\item Bias estimators of the Laplacian-Beltrami Operator $\lapop$.
			\item Bias the estimation of the intrinsic dimension $d$.
			\item Complicate neighborhood and bandwidth selection.
		\end{itemize}
		\item Most geometric methods assume or perform better with a uniform data distribution.
	\end{itemize}

	\textbf{Solution: Inverse-density weighted resampling}
	\begin{itemize}
		\item Estimate local density $\hat{p}(x)$ for $x \in \X[clean]$ using local statistics.
		\item Define sampling weights $\propto 1 / \hat{p}(x)$.
		\item Sample (without replacement) a subset $\X[unif]$ using these weights.
	\end{itemize}
	
\end{frame}


% ----------------------------------------------------------------------
% Step 6: Construct the Laplacian Matrix $\lapmat$
% ----------------------------------------------------------------------
\begin{frame}{Step 6: Construct the Laplacian Matrix $\lapmat$}

	\small
	
	\textbf{Construction of the affinity matrix $\affmat$}:
	\begin{itemize}
		\item On $\X[unif]$, build $\affmat$ by:
			\begin{itemize}
				\item Choosing a neighborhood scheme ($k$-NN or cutoff radius $r$),
				\item Defining edge weights using a kernel (e.g. Gaussian with bandwidth $\eps$).
			\end{itemize}
		\item \textbf{Strongly recommended}: Select bandwidth and cutoff scales by a geometry-preservation criterion(e.g.\ distortion minimization \citep{MeilaRMetric}).
	\end{itemize}

	\vspace{0.7em}

	\textbf{Construction of the Laplacian matrix $\lapmat$}:
	\begin{itemize}
		\item \textbf{Recommended}: Discard points with low degree to stabilize eigen-decompositions of $\lapmat$ further in the pipeline.
		\item Compute the renormalized Laplacian of \cite{CoifmanDM}.
	\end{itemize}

	\textbf{Important:} Both $\affmat$ and $\lapmat$ should be sparse matrices to save resources.

\end{frame}

% ----------------------------------------------------------------------
% Step 7: Manifold Learning: \diffmap
% ----------------------------------------------------------------------
% TODO: Could show eigenvalue spectrum to verify smooth, connected manifold
\begin{frame}{Step 7: Manifold Learning: \diffmap \citep{CoifmanDM}}

	\small
	\textbf{Goal:} Compute low-dimensional embedding coordinates $\Phi$ of the data points.

	\begin{itemize}
		\item Compute the eigen-decomposition of $\lapmat$.
		\item Choose the embedding dimension $m$ to comfortably larger than the intrinsic dimension $d$.
		\item Keep theRecompute $m$ smallest non-trivial eigenvalues and corresponding eigenvectors: $\lambda_{1:m}, \Phi_{1:m}$. 
		\item Interpreting the spectrum:
	\end{itemize}

	\textbf{Interpreting the spectrum, $\lambda_{1:m}$:}
	\begin{itemize}
		\item Single zero eigenvalue $\Rightarrow$ connected manifold,
		\item Slow growth of eigenvalues $\Rightarrow$ smooth geometry.
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% Step 8: Estimate Intrinsic Dimension $d$
% ----------------------------------------------------------------------
% ----------------------------------------------------------------------
\begin{frame}{Step 8: Estimate Intrinsic Dimension $d$}

	\small

	\textbf{Use multiple estimators to estimate the intrinsic dimension $d$:}
	\begin{itemize}
	
		\item \textbf{Correlation dimension} \citep{ProcacciaSlope}
			\begin{itemize}
				\item Scaling of average neighbor counts with radius $r$: $\log \nr[x] \propto d \cdot \log r$.
				\item Global estimate: $\hat{d} = \text{slope of } \log \overline{\nr[x]} \text{ vs. } \log r$.
			\end{itemize}
		\item \textbf{Doubling dimension} \citep{AssouadDoubling}
			\begin{itemize}
				\item By the same principle we have the local estimate $\hat{d}_r(x) = \log_2 \bigl(\nr[x][2r]/\nr[x][r]\bigr)$.
			\end{itemize}
		\item \textbf{Eigengap method} \citep{MaggioniEigenGap}
			\begin{itemize}
				\item Compute local covariance matrices and their eigen-decompositions.
				\item Local estimates: $\hat{d}(x) = \arg\max_{d} \lambda_{d} - \lambda_{d+1}$.
			\end{itemize}
		\item \textbf{MLE-based estimators} \citep{BickelLB}
			\begin{itemize}
				\item Use scaling of $\dk[x]$ for the top $k$-NN to locally estimate $d$ as:
				$\hat{d}_k(x) = \left[ \frac{1}{k-1} \sum_{j=1}^{k-1} \log \left(\frac{\dk[x][k]}{\dk[x][j]}\right) \right]^{-1}$.
			\end{itemize}
	\end{itemize}

\end{frame}

% ----------------------------------------------------------------------
% TODO: Add figures from paper showing stability of estimates across parameter ranges
\begin{frame}{Intrinsic Dimensionality Estimation Considerations}
	
	\small

	\textbf{Important:} Because many estimators of the intrinsic dimension $d$ require $\nr[x]$ or $\dk[x]$, we need to recompute(again!) them on $\X[unif]$.

	\textbf{Robustness:}
	\begin{itemize}
		\item For local estimators, we can average estimates $\hat{d}(x)$ to obtain a global estimate $\hat{d}$.
		\item For local estimators also depending on $k$ or $r$, we can further average over a range of values for $k$ or $r$.
		\item For estimators depending on $k$ or $r$, we need to select ranges $k_{\min}, k_{\max}$ and $r_{\min}, r_{\max}$ that have stable estimates.
	\end{itemize}
\end{frame}

% ----------------------------------------------------------------------
% Summary
% ----------------------------------------------------------------------
\begin{frame}{Summary: Intrinsic Dimensionality Estimation}
	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cGreen!15}{\textbf{Step 8.} Estimate intrinsic dimension $d$.}
			\end{itemize}
		\end{column}
		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Key points:}
			\begin{itemize}
				\item \textbf{Uniform resampling} is crucial for unbiased and reliable dimension and Laplacian estimation.
				\item Recompute neighbor statistics (\(\nr[x]\), \(\dk[x]\)) whenever the data is modified.
				\item Apply \textbf{multiple estimators} to robustly estimate intrinsic dimension $d$.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}


% ======================================================================
% Manifold Interpretation and Visualization
% ======================================================================
% ----------------------------------------------------------------------
% Overview
% ----------------------------------------------------------------------
\begin{frame}{Manifold Interpretation \& Visualization: Overview}
	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8.} Selecting Independent Coordinates: \ies*.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 9.} Identifying Manifold Parametrization I: Visualization.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 10.} Identifying Manifold Parametrization II: \tslasso.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 11.} Improving Visualization: \rrelax.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}

			\small
			
			\textbf{Goals:} 
			\begin{itemize}
				\item Reduce the dimensionality of spectral embeddings to a few interpretable and visualizable coordinates.
				\item Identify which generative parameters or factors drive the geometry of the data manifold.
			\end{itemize}

			\textbf{Why this matters for SBI:}
			\begin{itemize}
				\item Validate that the real data manifold is parametrized by simulation parameters.
				\item If a neural representation is used, validate that it is:
				\begin{itemize}
					\item Sensitive to the simulation parameters important for posterior inference.
					\item Not sensitive to nuisance variation or noise.
				\end{itemize}
			\end{itemize}
		\end{column}

	\end{columns}
\end{frame}

% ----------------------------------------------------------------------
% Step 8: Selecting Independent Coordinates: \ies*
% ----------------------------------------------------------------------
\begin{frame}{Step 8: Selecting Independent Coordinates with \ies*}

	\small
	
	\textbf{Problem:}
	\begin{itemize}
		\item Spectral embeddings often contain \emph{dependent} coordinates.
		\item Plotting all coordinates can be misleading and redundant.
		\item Example: $(x, \sin x)$ varies in two directions but has only one degree of freedom.
	\end{itemize}

	\textbf{\ies(\ies*) \citep{MeilaIES}:}
	\begin{itemize}
		\item Input: initial embedding $\Phi$ from \diffmap.
		\item Output: subset $S$ of coordinates such that $\Phi_S$:
			\begin{itemize}
				\item Is locally full-rank (avoids degeneracies).
				\item Uses as few low-frequency (smooth) coordinates as possible.
			\end{itemize}
	\item Provides a principled way to choose which coordinates to visualize and analyze without losing information and avoiding redundancy.
	\end{itemize}
\end{frame}


% ----------------------------------------------------------------------
% Step 9: Visualization for Interpretation
% ----------------------------------------------------------------------
\begin{frame}{Step 9: Identifying Manifold Parametrization I: Visualization}

	\small

	\textbf{Goal:} Use the selected coordinates $\Phi_S$ to visually probe the geometry of the data manifold.
		
	\textbf{Typical workflow:}
	\begin{itemize}
		\item Scatter plots in 2-3D using combinations of the coordinates selected with \ies*.
		\item Color points by simulation parameters.
	\end{itemize}
	
	\textbf{What to look for:}
	\begin{itemize}
		\item Smooth variation with respect to generative parameters relevant for inference.
		\item Clear structure (curves, surfaces, trajectories) rather than noise.
		\item Artifacts: holes, disconnected components, folding, or distortions.
	\end{itemize}

	\textbf{Important:} The experimental data manifold does not have ground truth generative parameters! 
	We `interpolate` them from those of the simulation data points using methods like kernel regression or the Nystrom Extension.
\end{frame}

% ----------------------------------------------------------------------
% Step 10: TSLasso
% ----------------------------------------------------------------------
\begin{frame}{Step 10: Identifying Manifold Parametrization II: \tslasso}
	\small
	
	\textbf{Problem:} When $m$ or $d$ exceed 3 or when many candidate coordinates are available, visual inspection becomes ambiguous, unreliable, and time-consuming.

	\textbf{Goal:} Provide a systematic, quantitative alternative to visualization for identifying manifold parametrizations.

	\textbf{Formally stated:}
	\begin{itemize}
		\item Given a \textbf{dictionary} of candidate coordinate functions $\mathcal{F} = \{f_k\}_{1:p}$ on the data.
		\item Find a subset of size $d$ of functions from $\mathcal{F}$ that \emph{parametrizes the manifold geometry}.
	\end{itemize}

	\textbf{Practical notes:}
	\begin{itemize}
		\item For SBI, $\mathcal{F}$ is the set of simulation parameters.
		\item The simulation parameters are obtained via interpolation from the experimental data.
	\end{itemize}

	\textbf{Solution: \tslasso* \citep{MeilaTSLasso}}

\end{frame}
  
% ----------------------------------------------------------------------
% TODO: Add figure from paper showing the output
\begin{frame}{Step 10: \tslasso: How it Works}

	\small

	\textbf{Main insight:} $d$ functions parametrize the manifold if their gradients $\nabla f_k$ span the tangent space at each point.

	\textbf{Method overview:}
	\begin{itemize}
		\item Estimate local tangent spaces from the data via local PCA.
		\item Estimate gradients $\nabla f_k$ of candidate functions from local neighborhoods.
		\item Project gradients onto the estimated tangent spaces.
		\item Reconstruct each tangent space as a sparse linear combination of projected gradients.
		\item Use a group-lasso penalty to select a small subset of influential functions.
	\end{itemize}

	\textbf{How to read the output:}
	\begin{itemize}
		\item Functions consistently selected across regularization levels are likely true parametrizing factors of the learned representation.
		\item Functions rarely or never selected are likely nuisance parameters or noise.
	\end{itemize}
\end{frame}
  
% ----------------------------------------------------------------------
% Step 11: Riemannian Relaxation
% ----------------------------------------------------------------------
% TODO: Add figures.
\begin{frame}{Step 11: Improving Visualization with \rrelax}

	\small

	\textbf{Motivation:} Embeddings from most manifold learning methods are not isometric. 
	That is, they do not preserve distances and angles and can obscure interpretation and visualization.
	
	\textbf{\rrelax(\rrelax*) \citep{MeilaRRelax}:} 
	\begin{itemize}
		\item Start from an embedding such as $\Phi_S$ after \diffmap*.
		\item Define a loss penalizing local metric distortions.
		\item Iteratively optimize the embedding to better preserve local distances and angles,
		\item Move closer to an isometric parametrization.
	\end{itemize}

	\textbf{End result:} A refined embedding that more faithfully reflects the geometry implied by the learned representation.

\end{frame}

% ----------------------------------------------------------------------
% Summary
% ----------------------------------------------------------------------
\begin{frame}{Summary: Manifold Interpretation \& Visualization}
	\begin{columns}[t]
		\begin{column}[t]{0.3\textwidth}
			\tiny
			\begin{itemize}
				\setlength\itemsep{1pt}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 1.} Construct Distance Matrix $\distmat$.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 2.} Compute Local Statistics.}
				\item[]
				\stepbox{cBlue!15}{\textbf{Step 3.} Outlier Removal.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 4.} Recompute Local Statistics.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 5.} Uniform Resampling.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 6.} Construct the Laplacian Matrix $\lapmat$.}
				\item[]
				\stepbox{cRed!15}{\textbf{Step 7.} Manifold Learning: \diffmap.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 8.} Selecting Independent Coordinates: \ies*.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 9.} Identifying Manifold Parametrization I: Visualization.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 10.} Identifying Manifold Parametrization II: \tslasso.}
				\item[]
				\stepbox{cOrange!15}{\textbf{Step 11.} Improving Visualization: \rrelax.}
			\end{itemize}
		\end{column}

		\begin{column}[t]{0.7\textwidth}
			\small
			\textbf{Key takeaways:}
			\begin{itemize}
				\item Provide tools for visual interpretation of high-dimensional data living on low-dimensional manifolds.
				\item Validate that the simulation parameters are the true generative factors of the experimental data.
				\item \ies: choose non-redundant, informative coordinates.
				\item Visualization: reveals correlations between simulation parameters and the data geometry.
				\item \tslasso: quantitatively tests which factors parametrize the manifold.
				\item \rrelax: optional refinement for clearer, more faithful embeddings.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}