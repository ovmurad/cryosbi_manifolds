@article{AssouadDoubling,
	author = {Assouad, Patrice},
	title = {Plongements lipschitziens dans ${\mathbb {R}}^n$},
	journal = {Bulletin de la Soci\'et\'e Math\'ematique de France},
	pages = {429--448},
	publisher = {Soci\'et\'e math\'ematique de France},
	volume = {111},
	year = {1983},
	doi = {10.24033/bsmf.1997},
	mrnumber = {86f:54050},
	zbl = {0597.54015},
	language = {fr},
	url = {http://www.numdam.org/articles/10.24033/bsmf.1997/}
}

@inproceedings{BickelLevinaBickel,
	author = {Levina, Elizaveta and Bickel, Peter},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	pages = {},
	publisher = {MIT Press},
	title = {Maximum Likelihood Estimation of Intrinsic Dimension},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf},
	volume = {17},
	year = {2004}
}

@article{CoifmanDM,
	title = {Diffusion maps},
	journal = {Applied and Computational Harmonic Analysis},
	volume = {21},
	number = {1},
	pages = {5-30},
	year = {2006},
	note = {Special Issue: Diffusion Maps and Wavelets},
	issn = {1063-5203},
	doi = {https://doi.org/10.1016/j.acha.2006.04.006},
	url = {https://www.sciencedirect.com/science/article/pii/S1063520306000546},
	author = {Ronald R. Coifman and St√©phane Lafon},
	keywords = {Diffusion processes, Diffusion metric, Manifold learning, Dimensionality reduction, Eigenmaps, Graph Laplacian},
	abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.}
}

@article{DingeldeinCryoSBI,
	author = {Dingeldein, Lars and Silva-S{\'a}nchez, David and Evans, Luke and D{\textquoteright}Imprima, Edoardo and Grigorieff, Nikolaus and Covino, Roberto and Cossio, Pilar},
	title = {Amortized template-matching of molecular conformations from cryo-electron microscopy images using simulation-based inference},
	elocation-id = {2024.07.23.604154},
	year = {2024},
	doi = {10.1101/2024.07.23.604154},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Biomolecules undergo conformational changes to perform their function. Cryo-electron microscopy (cryo-EM) can capture snapshots of biomolecules in various conformations. However, these images are noisy and display the molecule in unknown orientations, making it difficult to separate conformational differences from differences due to noise or projection directions. Here, we introduce cryo-EM simulation-based inference (cryoSBI) to infer the conformations of biomolecules and the uncertainties associated with the inference from individual cryo-EM images. CryoSBI builds on simulation-based inference, a combination of physics-based simulations and probabilistic deep learning, allowing us to use Bayesian inference even when likelihoods are too expensive to calculate. We begin with an ensemble of conformations, which can be templates from molecular simulations or modelling, and use them as structural hypotheses. We train a neural network approximating the Bayesian posterior using simulated images from these templates, and then use it to accurately infer the conformations of biomolecules from experimental images. Training is only done once, and after that, it takes just a few milliseconds to evaluate on an image, making cryoSBI suitable for arbitrarily large datasets. This method eliminates the need to estimate particle pose and imaging parameters, significantly enhancing the computational speed in comparison to explicit likelihood methods. We illustrate and benchmark cryoSBI on synthetic data and showcase its promise on experimental single-particle cryo-EM data.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2024/07/24/2024.07.23.604154},
	eprint = {https://www.biorxiv.org/content/early/2024/07/24/2024.07.23.604154.full.pdf},
	journal = {bioRxiv}
}

@Inbook{MaggioniEigenGap,
	author = {Chen, Guangliang and Little, Anna V. and Maggioni, Mauro},
	title = {Multi-Resolution Geometric Analysis for Data in High Dimensions},
	bookTitle = {Excursions in Harmonic Analysis, Volume 1: The February Fourier Talks at the Norbert Wiener Center",
	year="2013},
	publisher = {Birkh{\"a}user Boston},
	address = {Boston},
	pages = {259--285},
	isbn = {978-0-8176-8376-4},
	doi = {10.1007/978-0-8176-8376-4_13},
	url = {https://doi.org/10.1007/978-0-8176-8376-4_13}
}

@article{MeilaCoordsWithMeaning,
	title={Manifold coordinates with physical meaning},
	author={Koelle, Samson J and Zhang, Hanyu and Meila, Marina and Chen, Yu-Chia},
	journal={Journal of Machine Learning Research},
	volume={23},
	number={133},
	pages={1--57},
	year={2022}
}


@article{MeilaIES,
	title={Selecting the independent coordinates of manifolds with large aspect ratios},
	author={Chen, Yu-Chia and Meila, Marina},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}

@article{MeilaManifoldReview,
	title={Manifold learning: What, how, and why},
	author={Meil{\u{a}}, Marina and Zhang, Hanyu},
	journal={Annual Review of Statistics and Its Application},
	volume={11},
	year={2024},
	publisher={Annual Reviews}
}

@article{MeilaRMetric,
	title={Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery},
	author={Perraul-Joncas, Dominique and Meila, Marina},
	journal={arXiv preprint arXiv:1305.7255},
	year={2013}
}

@inproceedings{MeilaRRelax,
	author = {McQueen, James and Meila, Marina and Joncas, Dominique},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Nearly Isometric Embedding by Relaxation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/cf1f78fe923afe05f7597da2be7a3da8-Paper.pdf},
	volume = {29},
	year = {2016}
}

@inproceedings{MeilaTSLasso,
	title={Consistency of Dictionary-Based Manifold Learning},
	author={Koelle, Samson J and Zhang, Hanyu and Murad, Octavian-Vlad and Meila, Marina},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={4348--4356},
	year={2024},
	organization={PMLR}
}

@article{ProcacciaSlope,
	title = {Measuring the strangeness of strange attractors},
	journal = {Physica D: Nonlinear Phenomena},
	volume = {9},
	number = {1},
	pages = {189-208},
	year = {1983},
	issn = {0167-2789},
	doi = {https://doi.org/10.1016/0167-2789(83)90298-1},
	url = {https://www.sciencedirect.com/science/article/pii/0167278983902981},
	author = {Peter Grassberger and Itamar Procaccia},
	abstract = {We study the correlation exponent v introduced recently as a characteristic measure of strange attractors which allows one to distinguish between deterministic chaos and random noise. The exponent v is closely related to the fractal dimension and the information dimension, but its computation is considerably easier. Its usefulness in characterizing experimental data which stem from very high dimensional systems is stressed. Algorithms for extracting v from the time series of a single variable are proposed. The relations between the various measures of strange attractors and between them and the Lyapunov exponents are discussed. It is shown that the conjecture of Kaplan and Yorke for the dimension gives an upper bound for v. Various examples of finite and infinite dimensional systems are treated, both numerically and analytically.}
}
